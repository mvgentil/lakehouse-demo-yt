{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üóÇÔ∏è Explorando o Data Lake (Parquet)\n",
                "\n",
                "Neste notebook vamos explorar os dados armazenados no **Data Lake** (arquivos Parquet puros).\n",
                "\n",
                "O objetivo √© demonstrar como funciona um Data Lake tradicional:\n",
                "- Sem transa√ß√µes ACID\n",
                "- Sem versionamento\n",
                "- Sem schema enforcement\n",
                "- Leitura direta dos arquivos `.parquet`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import duckdb\n",
                "import os\n",
                "\n",
                "# Paths\n",
                "BASE_DIR = os.path.abspath('..')\n",
                "BRONZE = os.path.join(BASE_DIR, 'data', 'lake', '01_bronze')\n",
                "SILVER = os.path.join(BASE_DIR, 'data', 'lake', '02_silver')\n",
                "GOLD = os.path.join(BASE_DIR, 'data', 'lake', '03_gold')\n",
                "\n",
                "con = duckdb.connect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Bronze Layer\n",
                "Dados brutos convertidos de CSV para Parquet, sem nenhuma transforma√ß√£o."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verificar os arquivos na pasta Bronze\n",
                "print('Arquivos em Bronze:')\n",
                "for f in os.listdir(BRONZE):\n",
                "    size_mb = os.path.getsize(os.path.join(BRONZE, f)) / (1024 * 1024)\n",
                "    print(f'  {f} ({size_mb:.2f} MB)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ler e explorar o Parquet da Bronze\n",
                "bronze_path = os.path.join(BRONZE, 'online_retail.parquet')\n",
                "\n",
                "con.execute(f\"\"\"\n",
                "    SELECT * FROM read_parquet('{bronze_path}') LIMIT 5\n",
                "\"\"\").df()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Schema dos dados\n",
                "con.execute(f\"\"\"\n",
                "    DESCRIBE SELECT * FROM read_parquet('{bronze_path}')\n",
                "\"\"\").df()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Contagem de registros\n",
                "con.execute(f\"\"\"\n",
                "    SELECT COUNT(*) as total_rows FROM read_parquet('{bronze_path}')\n",
                "\"\"\").df()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Silver Layer\n",
                "Dados limpos e modelados em Star Schema (Fato + Dimens√µes)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verificar arquivos na Silver\n",
                "print('Arquivos em Silver:')\n",
                "for f in os.listdir(SILVER):\n",
                "    size_mb = os.path.getsize(os.path.join(SILVER, f)) / (1024 * 1024)\n",
                "    print(f'  {f} ({size_mb:.2f} MB)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fact Sales\n",
                "fact_path = os.path.join(SILVER, 'fact_sales.parquet')\n",
                "con.execute(f\"SELECT * FROM read_parquet('{fact_path}') LIMIT 5\").df()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dim Customer\n",
                "cust_path = os.path.join(SILVER, 'dim_customer.parquet')\n",
                "con.execute(f\"SELECT * FROM read_parquet('{cust_path}') LIMIT 5\").df()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dim Product\n",
                "prod_path = os.path.join(SILVER, 'dim_product.parquet')\n",
                "con.execute(f\"SELECT * FROM read_parquet('{prod_path}') LIMIT 5\").df()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Contagem de registros por tabela\n",
                "for table in ['fact_sales', 'dim_customer', 'dim_product']:\n",
                "    path = os.path.join(SILVER, f'{table}.parquet')\n",
                "    count = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{path}')\").fetchone()[0]\n",
                "    print(f'{table}: {count:,} registros')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Gold Layer\n",
                "Dados agregados prontos para consumo anal√≠tico."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Daily Sales\n",
                "daily_path = os.path.join(GOLD, 'daily_sales.parquet')\n",
                "con.execute(f\"SELECT * FROM read_parquet('{daily_path}') LIMIT 10\").df()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sales by Country\n",
                "country_path = os.path.join(GOLD, 'sales_by_country.parquet')\n",
                "con.execute(f\"SELECT * FROM read_parquet('{country_path}') LIMIT 10\").df()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚ö†Ô∏è Limita√ß√µes do Data Lake (Parquet Puro)\n",
                "\n",
                "Vamos demonstrar algumas limita√ß√µes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Sem versionamento / time travel\n",
                "# Se sobrescrevermos o arquivo, perdemos os dados antigos\n",
                "print('No Data Lake n√£o existe conceito de vers√£o.')\n",
                "print('Se sobrescrevermos o parquet, os dados antigos s√£o perdidos.')\n",
                "print('N√£o h√° como fazer \"SELECT * FROM tabela VERSION AS OF 1\"')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Sem schema enforcement ‚Äî qualquer dado pode ser escrito\n",
                "import pandas as pd\n",
                "\n",
                "df_wrong_schema = pd.DataFrame({\n",
                "    'coluna_errada': [1, 2, 3],\n",
                "    'outra_coluna': ['a', 'b', 'c']\n",
                "})\n",
                "\n",
                "# Isto vai funcionar! O Parquet aceita qualquer schema.\n",
                "wrong_path = os.path.join(BRONZE, 'dados_errados.parquet')\n",
                "df_wrong_schema.to_parquet(wrong_path, index=False)\n",
                "print(f'Arquivo com schema errado escrito em: {wrong_path}')\n",
                "print('O Data Lake n√£o impede a escrita de dados com schema diferente!')\n",
                "\n",
                "# Limpeza\n",
                "os.remove(wrong_path)\n",
                "print('(Arquivo removido para manter a pasta limpa)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Sem transa√ß√µes ACID\n",
                "print('Se o processo falhar no meio da escrita, o arquivo fica corrompido.')\n",
                "print('N√£o h√° rollback autom√°tico no Data Lake.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
